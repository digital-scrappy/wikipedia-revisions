#+Title: Documentation for various helper functions
* preliminary results


#+begin_quote
avg. rho: emp -> 0.36473591446118697
avg. pval: emp -> 0.41395059623960395

avg. rho: wage -> 0.40135077440409767
avg. pval: wage -> 0.36451839945463854

avg. rho: emp_growth -> 0.315427998495521
avg. pval: emp_growth -> 0.49933698369676244

avg. rho: wage_growth -> 0.32272633278255625
avg. pval: wage_growth -> 0.4889306728227626
#+end_quote
* Technical debt
Locally saving all the Wikipedia data we could ever need for the 300 labelled pages would save me from ever struggling with ConnectionErrors again.
[[https://mwclient.readthedocs.io/en/latest/user/page-ops.html][mwclient]] should be the best solution for retrieving wikipedia data (although building something that can read a whole wikipedia dump could be a cool solution if on has space for a few TB temporarily).



* TODO look into de-trending
* TODO Qualitative analysis of edits
* TODO learn about following software
tomotopy
structural topic modelling in R
berttopic

* Questions and Observations
** edit frequency observations
*** Spike after Page inception?
The hypothesis:
The number of edit spikes after pages creation and then levels of as the page has gotten to an acceptable/complete state.

- Observation 1:
  This does not hold for pages like [[https://en.wikipedia.org/wiki/Baker][Baker]]. Baker was created in 2003 and recieved only around 10 edits in that year. Also in 2004 the number of edits was pretty low.

  Back then [[https://en.wikipedia.org/wiki/Baker][Baker]] was more similar to a disambiguation page today - the site also named quite some towns with baker in their name.
  
** Hypothesis: The Beginnings of Wikipedia and Pages are chaotic
Should we just give Wikipedia and Occupation pages some time to settle down? (1 or 2 years?)

** Watch out for stubs?
Is there a way to track stubs and should we only consider pages after loosing their stub status?

* Methodological notes
The estimates of the oews estimates are calculated for a specific month (in our case I selected only the ones for May) but they rely on the 6 most recent surveys (2 per year) to produce an estimate.

#+begin_quote
The May 2019 employment and wage estimates were calculated using data collected in the May 2019, November 2018, May 2018, November 2017, May 2017, and November 2016 semi-annual panels. 
--- https://www.bls.gov/oes/oes_ques.htm#overview
#+end_quote

Since we still have yearly estimates for labour statistic we use the edits accumulated edits in the 12 months up to and including the month of the estimate.
For May 2012 we count the edits starting with June 2011 ending with May 2012
* Data 
overall source for data:
https://www.bls.gov/oes/tables.htm
specific source link https://www.bls.gov/oes/special.requests/oesm21nat.zip
potentially better estimates for employment stats are here https://www.bls.gov/oes/oes-mb3-methods.htm
* Requirements
can be either installed via poetry or pip using `pip install -r requirements.txt`

* Important functions / files

** get_pages_for_occupations()
A function to retrieve the candidate links the top n link returned by wikipedia search for each occupation


Args:
 - in_path (str) : the path to the xlsx file holding the bls soc structure

 - db_path (str) : the path to the sqlite db storing the data

 - group_level (str): The the level at which to extract occupations can be one of either ["Minor  Group", "Broad Group", "Detailed Occupation"]

 - srlimit (int) : the number of top search results to use


Returns:
  None

 
** terminal_interface.py

a terminal programm for selecting from the candidate links

