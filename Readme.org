#+Title: Documentation for various helper functions
* Code specific Information
** Information and Usefull Software
I guess the most interesting scripts currently are located in =the Analysis/= directory
For now since the whole execution only takes a fraction of a second I am not using notebooks

*** Dependency management
I manage dependencies using poetry it's a python dependency management tool. 
But it's also straightforward to install the dependencies using =pip install -r requirements.txt=
If some dependencies are missing in the requirements.txt thats because I (jakob) forgot to run =poetry export= in that case write me an angry email :)

*** Sqlite utils
- sqlitebrowser
  for viewing the databases
 
** Wikipedia api
creating an OAuth consumer for higher request limit can be done here (after creating a WikiMedia account) https://meta.wikimedia.org/wiki/Special:OAuthConsumerRegistration/propose

** Requirements
can be either installed via poetry or pip using `pip install -r requirements.txt`

** Important functions / files

*** get_pages_for_occupations()
A function to retrieve the candidate links the top n link returned by wikipedia search for each occupation


Args:
 - in_path (str) : the path to the xlsx file holding the bls soc structure

 - db_path (str) : the path to the sqlite db storing the data

 - group_level (str): The the level at which to extract occupations can be one of either ["Minor  Group", "Broad Group", "Detailed Occupation"]

 - srlimit (int) : the number of top search results to use


Returns:
  None

 
*** terminal_interface.py

a terminal programm for selecting from the candidate links



** TODO learn about following software
tomotopy
structural topic modelling in R
berttopic

* Understanding Wikipedia
** Size of page content probably matters
For now I'll try to find an acceptable minimum page size
** Qualitative analysis of edits
** Spike after Page inception?
The hypothesis:
The number of edit spikes after pages creation and then levels of as the page has gotten to an acceptable/complete state.

- Observation 1:
  This does not hold for pages like [[https://en.wikipedia.org/wiki/Baker][Baker]]. Baker was created in 2003 and recieved only around 10 edits in that year. Also in 2004 the number of edits was pretty low.

  Back then [[https://en.wikipedia.org/wiki/Baker][Baker]] was more similar to a disambiguation page today - the site also named some towns with baker in their name.
  
** Hypothesis: The Beginnings of Wikipedia and Pages are chaotic
Should we just give Wikipedia and Occupation pages some time to settle down? (1 or 2 years?)

** Watch out for stubs?
Is there a way to track stubs and should we only consider pages after loosing their stub status?

* Methodological notes
The estimates of the oews estimates are calculated for a specific month (in our case I selected only the ones for May) but they rely on the 6 most recent surveys (2 per year) to produce an estimate.

#+begin_quote
The May 2019 employment and wage estimates were calculated using data collected in the May 2019, November 2018, May 2018, November 2017, May 2017, and November 2016 semi-annual panels. 
--- https://www.bls.gov/oes/oes_ques.htm#overview
#+end_quote

Since we still have yearly estimates for labour statistic we use the edits accumulated edits in the 12 months up to and including the month of the estimate.
For May 2012 we count the edits starting with June 2011 ending with May 2012
* Data 
overall source for data:
https://www.bls.gov/oes/tables.htm
specific source link https://www.bls.gov/oes/special.requests/oesm21nat.zip
potentially better estimates for employment stats are here https://www.bls.gov/oes/oes-mb3-methods.htm

** removing pages of innsufficent lenght
The 40th percentile lays around a page length of 10ky
